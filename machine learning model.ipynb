{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT CLASSIFICATION / TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Waqas\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "#from sklearn.decomposition import TruncatedSVDs\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.ensemble import ExtraTreesClassifier\n",
    "#from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('Training Data for Primary.xlsx',sheetname = 'Sheet1')\n",
    "data = data.sample(frac=1) #shuffle data\n",
    "data_pred = pd.read_excel('Primary to be decided.xlsx',sheetname = 'Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOOKING AT DATA **NON-STRATIFICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Existing PIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Application Software</th>\n",
       "      <td>31544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movies and Entertainment</th>\n",
       "      <td>4565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Systems Software</th>\n",
       "      <td>4388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interactive Media and Services</th>\n",
       "      <td>3179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Integrated Telecommunication Services</th>\n",
       "      <td>2410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data Processing and Outsourced Services</th>\n",
       "      <td>1918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Internet Services and Infrastructure</th>\n",
       "      <td>1732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Financial Exchanges and Data</th>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Internet and Direct Marketing Retail</th>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Existing PIC\n",
       "Application Software                            31544\n",
       "Movies and Entertainment                         4565\n",
       "Systems Software                                 4388\n",
       "Interactive Media and Services                   3179\n",
       "Integrated Telecommunication Services            2410\n",
       "Data Processing and Outsourced Services          1918\n",
       "Internet Services and Infrastructure             1732\n",
       "Financial Exchanges and Data                      818\n",
       "Internet and Direct Marketing Retail              108"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total_unique_values = pd.DataFrame(data['Existing PIC'].value_counts())\n",
    "# total_unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA AFTER DROPPING SPECIFIC NO OF SAMPLES IN SPECIFIC CLASS.<br>DONE TO MAKE NO OF SENTENCES VARY IN SPECIFIC CLASS.<br>HELPS IN MANUAL ERROR ANALYSIS AT END BY COMPARING F1 SCORES. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##decreasing data by removing 26000 APPLICATION SOFTWARE CLASS data points.\n",
    "# app_index = data[data['Existing PIC'] == 'Application Software'].index.tolist()\n",
    "# new_app_index = sample(app_index,26000)\n",
    "# data.drop(new_app_index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVING **STOP WORDS**, **PUNCTUATIONS**, **WHITESPACES** AND DOING **STEMMNG**, **LOWERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punc = string.punctuation\n",
    "stemmer = SnowballStemmer('english')\n",
    "words = stopwords.words(\"english\")\n",
    "punc = punc.replace(\"&\", \"\") # don't remove &\n",
    "pattern = r\"[{}]\".format(punc) # create the pattern\n",
    "\n",
    "data['cleaned'] = data['Business Description'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(pattern, \" \", x).split() if i not in words]).lower())\n",
    "#data['cleaned'] = data['cleaned'].apply(lambda x: ''.join(ch for ch in x  if ch not in pattern))\n",
    "data['cleaned'] = data['cleaned'].str.strip()\n",
    "\n",
    "data_pred['cleaned'] = data_pred['Business Description'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(pattern, \" \", x).split() if i not in words]).lower())\n",
    "#data['cleaned'] = data['cleaned'].apply(lambda x: ''.join(ch for ch in x  if ch not in pattern))\n",
    "data_pred['cleaned'] = data_pred['cleaned'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMOVING SENTENCES HAVING LENGTH > 50.<br>THIS REMOVAL DOES NOT MAKE MUCH DIFFERENCE BECAUSE ONLY ABOUT 100 SAMPLES ARE GREATER THAN LENGTH 50 AND MOSTLY ARE FROM APPLICATIONS SOFTWARE<br><font color=red>**IT IS DONE FOR FUTURE PADDING PURPOSES IN BIDIRECTIONAL LSTM CELL**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data['len'] = data['cleaned'].str.split()\n",
    "# data['len'] = data['len'].apply(lambda x: len(x))\n",
    "# large_sent = data[data['len']>50]\n",
    "# filter_data = data[data['len']<50]\n",
    "\n",
    "# short_sentences = filter_data['cleaned']\n",
    "# short_y = filter_data['Existing PIC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING DATA INPUTS AND OUTPUTS AFTER PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data['cleaned']\n",
    "y = data['Existing PIC']\n",
    "\n",
    "X_pred = data_pred['cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### common words between application software class<font color=red>(having 62% of data)</font> and all other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# app_soft = data[data['Existing PIC']== 'Application Software']\n",
    "# app_soft_counts = pd.Series(\" \".join(app_soft['cleaned']).split()).value_counts()[:100]\n",
    "# int_med = data[data['Existing PIC']== 'Interactive Media and Services']\n",
    "# int_med_counts = pd.Series(\" \".join(int_med['cleaned']).split()).value_counts()[:100]\n",
    "# sys_soft = data[data['Existing PIC']== 'Systems Software']\n",
    "# sys_soft_counts = pd.Series(\" \".join(sys_soft['cleaned']).split()).value_counts()[:100]\n",
    "# mov_ent = data[data['Existing PIC']== 'Movies and Entertainment']\n",
    "# mov_ent_counts = pd.Series(\" \".join(mov_ent['cleaned']).split()).value_counts()[:100]\n",
    "# data_pro = data[data['Existing PIC']== 'Data Processing and Outsourced Services']\n",
    "# data_pro_counts = pd.Series(\" \".join(data_pro['cleaned']).split()).value_counts()[:100]\n",
    "# int_ser = data[data['Existing PIC']== 'Internet Services and Infrastructure']\n",
    "# int_ser_counts = pd.Series(\" \".join(int_ser['cleaned']).split()).value_counts()[:100]\n",
    "# int_tel = data[data['Existing PIC']== 'Integrated Telecommunication Services']\n",
    "# int_tel_counts = pd.Series(\" \".join(int_tel['cleaned']).split()).value_counts()[:100]\n",
    "# fin_exc = data[data['Existing PIC']== 'Financial Exchanges and Data']\n",
    "# fin_exc_counts = pd.Series(\" \".join(fin_exc['cleaned']).split()).value_counts()[:100]\n",
    "# int_dir = data[data['Existing PIC']== 'Internet and Direct Marketing Retail']\n",
    "# int_dir_counts = pd.Series(\" \".join(int_dir['cleaned']).split()).value_counts()[:100]\n",
    "\n",
    "# common_int = pd.DataFrame({'int':int_med_counts,'app':app_soft_counts})\n",
    "# common_sys = pd.DataFrame({'sys':sys_soft_counts,'app':app_soft_counts})\n",
    "# common_mov = pd.DataFrame({'mov':mov_ent_counts,'app':app_soft_counts})\n",
    "# common_data = pd.DataFrame({'data':data_pro_counts,'app':app_soft_counts})\n",
    "# common_ser = pd.DataFrame({'ser':int_ser_counts,'app':app_soft_counts})\n",
    "# common_tel = pd.DataFrame({'tel':int_tel_counts,'app':app_soft_counts})\n",
    "# common_fin = pd.DataFrame({'fin':fin_exc_counts,'app':app_soft_counts})\n",
    "# common_dir = pd.DataFrame({'dir':int_dir_counts,'app':app_soft_counts})\n",
    "\n",
    "# Systems_Soft = common_sys.dropna(thresh=2).shape[0]\n",
    "# Interactive_Media = common_int.dropna(thresh=2).shape[0]\n",
    "# Movies_and_Enter = common_mov.dropna(thresh=2).shape[0]\n",
    "# Data_Processing_and_Outsourced = common_data.dropna(thresh=2).shape[0]\n",
    "# Internet_Services = common_ser.dropna(thresh=2).shape[0]\n",
    "# Integrated_Telecommunication = common_tel.dropna(thresh=2).shape[0]\n",
    "# Financial_Exchanges = common_fin.dropna(thresh=2).shape[0]\n",
    "# Internet_and_Direct = common_dir.dropna(thresh=2).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### out of 100 most frequent words in all classes following are common no of words between application software class and specified class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Data_Processing_and_Outsourced_Services': 69,\n",
       " 'Financial_Exchanges_and_Data': 51,\n",
       " 'Integrated_Telecommunication_Services': 49,\n",
       " 'Interactive_Media_and_Services': 63,\n",
       " 'Internet_Services_and_Infrastructure': 68,\n",
       " 'Internet_and_Direct_Marketing_Retail': 45,\n",
       " 'Movies_and_Entertainment': 41,\n",
       " 'Systems_Software': 75}"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# common_values = {'Systems_Software':Systems_Soft,\n",
    "#                 'Interactive_Media_and_Services':Interactive_Media,\n",
    "#                 'Movies_and_Entertainment':Movies_and_Enter,\n",
    "#                 'Data_Processing_and_Outsourced_Services':Data_Processing_and_Outsourced,\n",
    "#                 'Internet_Services_and_Infrastructure':Internet_Services,\n",
    "#                 'Integrated_Telecommunication_Services':Integrated_Telecommunication,\n",
    "#                 'Financial_Exchanges_and_Data':Financial_Exchanges,\n",
    "#                 'Internet_and_Direct_Marketing_Retail':Internet_and_Direct}\n",
    "common_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRE-TRAINED WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TWO METHODS ARE USED TO EXTRACT SENTENCE VECTOR OUT OF WORD VECTORS:<br>\n",
    "<font color=red>**1. TAKING MEAN OF ALL VECTORS.**<br>\n",
    "**2. TAKING MAX,MIN OF ALL VECTORS AND CONCATENATING THEM**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class GloveVectorizer:\n",
    "#     def __init__(self):# load in pre-trained word vectors\n",
    "#         print('Loading word vectors...')\n",
    "#         word2vec = {}\n",
    "#         embedding = []\n",
    "#         idx2word = []\n",
    "#         with open('glove.6B.300d.txt',encoding= 'utf-8') as f:\n",
    "#             # is just a space-separated text file in the format:\n",
    "#           # word vec[0] vec[1] vec[2] ...\n",
    "#           for line in f:\n",
    "#                 values = line.split()\n",
    "#                 word = values[0]\n",
    "#                 vec = np.asarray(values[1:], dtype='float32')\n",
    "#                 word2vec[word] = vec\n",
    "#                 embedding.append(vec)\n",
    "#                 idx2word.append(word)\n",
    "#                 #print('Found %s word vectors.' % len(word2vec)) for visualization\n",
    "\n",
    "#                 # save for later\n",
    "#                 self.word2vec = word2vec\n",
    "#                 self.embedding = np.array(embedding)\n",
    "#                 self.word2idx = {v:k for k,v in enumerate(idx2word)}\n",
    "#                 self.V, self.D = self.embedding.shape\n",
    "\n",
    "#     def fit(self, data):\n",
    "#         pass\n",
    "\n",
    "#     def transform(self, data):\n",
    "#         X = np.zeros((len(data), self.D))\n",
    "#         n = 0\n",
    "#         emptycount = 0\n",
    "#         for sentence in data:\n",
    "#             tokens = sentence.lower().split()\n",
    "#             vecs = []\n",
    "#             for word in tokens:\n",
    "#                 if word in self.word2vec:\n",
    "#                     vec = self.word2vec[word]\n",
    "#                     vecs.append(vec)\n",
    "#             if len(vecs) > 0:\n",
    "#                 vecs = np.array(vecs)\n",
    "#                 max_vec = vecs.max(axis=0)\n",
    "#                 #min_vec = vecs.min(axis=0)\n",
    "#                 #X[n] = vecs.mean(axis=0)\n",
    "#                 #X[n] = np.concatenate((max_vec,min_vec))\n",
    "#                 X[n] = max_vec\n",
    "#             else:\n",
    "#                 emptycount += 1\n",
    "#             n += 1\n",
    "#         print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "#         return X\n",
    "\n",
    "#     def fit_transform(self, data):\n",
    "#         self.fit(data)\n",
    "#         return self.transform(data)\n",
    "\n",
    "# glove_vectorizer = GloveVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Word2VecVectorizer:\n",
    "#     def __init__(self):\n",
    "#         print(\"Loading in word vectors...\")\n",
    "#         self.word_vectors = KeyedVectors.load_word2vec_format(\n",
    "#         'GoogleNews-vectors-negative300.bin',binary=True\n",
    "#             )\n",
    "#         print(\"Finished loading in word vectors\")\n",
    "        \n",
    "#     def fit(self, data):\n",
    "#         pass\n",
    "\n",
    "#     def transform(self, data):\n",
    "#         # determine the dimensionality of vectors\n",
    "#         v = self.word_vectors.get_vector('king')\n",
    "#         self.D = v.shape[0]\n",
    "\n",
    "#         X = np.zeros((len(data), self.D))\n",
    "#         n = 0\n",
    "#         emptycount = 0\n",
    "#         for sentence in data:\n",
    "#             tokens = sentence.split()\n",
    "#             vecs = []\n",
    "#             m = 0\n",
    "#             for word in tokens:\n",
    "#                 try:\n",
    "#                     # throws KeyError if word not found\n",
    "#                     vec = self.word_vectors.get_vector(word)\n",
    "#                     vecs.append(vec)\n",
    "#                     m += 1\n",
    "#                 except KeyError:\n",
    "#                     pass\n",
    "#             if len(vecs) > 0:\n",
    "#                 vecs = np.array(vecs)\n",
    "#                 max_vec = vecs.max(axis=0)\n",
    "#                 #min_vec = vecs.min(axis=0)\n",
    "#                 #X[n] = vecs.mean(axis=0)\n",
    "#                 #X[n] = np.concatenate((max_vec,min_vec))\n",
    "#                 X[n] = max_vec\n",
    "#             else:\n",
    "#                 emptycount += 1\n",
    "#             n += 1\n",
    "#         print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "#         return X\n",
    "\n",
    "\n",
    "#       def fit_transform(self, data):\n",
    "#             self.fit(data)\n",
    "#             return self.transform(data)\n",
    "\n",
    "# word_vectorizer = Word2VecVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USING GLOVE/WORD2VEC AFTER LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = glove_vectorizer.fit_transform(X)\n",
    "#X = word_vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COUNTER / TF-IDF VECTORIZERS USING N-GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### COUNTER\n",
    "vect = CountVectorizer(ngram_range=(1, 3), stop_words=\"english\")\n",
    "vect.fit(X)\n",
    "X = vect.transform(X)\n",
    "\n",
    "X_pred = vect.transform(X_pred)\n",
    "\n",
    "# ### TFIDF\n",
    "# #vect = TfidfVectorizer(ngram_range=(1, 1))\n",
    "# #vect.fit(X)\n",
    "# #X = vect.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USING CHI DISTRIBUTION/SVD ETC FOR TOP 20000 FEATURES SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50662x20000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 725760 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi = SelectKBest(chi2,20000)\n",
    "chi.fit_transform(X,y)\n",
    "\n",
    "#svd = TruncatedSVD(20000)\n",
    "#X = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  HANDLING NON-STRATIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sm = SMOTE()\n",
    "#X, y = SMOTE(kind = 'svm').fit_sample(X,y)\n",
    "#class_weights = {'Application Software':0.1,'Movies and Entertainment':1,'Systems Software':10,\n",
    "#                 'Interactive Media and Services':1,'Integrated Telecommunication Services':10,\n",
    "#                'Data Processing and Outsourced Services':1,'Internet Services and Infrastructure':1,\n",
    "#                'Financial Exchanges and Data':1,'Internet and Direct Marketing Retail':3}\n",
    "#THESE CLASS WEIGHTS ARE USED IN LINEARSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LinearSVC(penalty='l1',dual=False)            \n",
    "model = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKING PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_train = model.predict(X)\n",
    "data_pred['Proposed PIC'] = model.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999802603718465"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(predicted_train,y,average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Business Description</th>\n",
       "      <th>CIQ Company ID</th>\n",
       "      <th>Proposed PIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Burgiss Group, LLC develops and offers inv...</td>\n",
       "      <td>19351</td>\n",
       "      <td>Financial Exchanges and Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lycos, Inc. is an Internet company that operat...</td>\n",
       "      <td>21563</td>\n",
       "      <td>Interactive Media and Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Providers of online services for IT profession...</td>\n",
       "      <td>21871</td>\n",
       "      <td>Interactive Media and Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SkyDesk, Inc. provides online data storage and...</td>\n",
       "      <td>24078</td>\n",
       "      <td>Internet Services and Infrastructure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The company provides an Internet based browse...</td>\n",
       "      <td>24159</td>\n",
       "      <td>Application Software</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Business Description  CIQ Company ID  \\\n",
       "0  The Burgiss Group, LLC develops and offers inv...           19351   \n",
       "1  Lycos, Inc. is an Internet company that operat...           21563   \n",
       "2  Providers of online services for IT profession...           21871   \n",
       "3  SkyDesk, Inc. provides online data storage and...           24078   \n",
       "4   The company provides an Internet based browse...           24159   \n",
       "\n",
       "                           Proposed PIC  \n",
       "0          Financial Exchanges and Data  \n",
       "1        Interactive Media and Services  \n",
       "2        Interactive Media and Services  \n",
       "3  Internet Services and Infrastructure  \n",
       "4                  Application Software  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pred.drop('cleaned',axis = 1,inplace = True)\n",
    "data_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('Predicted Results.xlsx')\n",
    "data_pred.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFUSION MATRIX IS BEST FOR MANUAL ERROR ANALYSIS.<br>\n",
    "LOOKING AT CONFUSION MATRIX FURTHER HYPER-HYPER PARAMETERS COULD BE TUNED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# con_mat = pd.DataFrame(confusion_matrix(predicted_test,y_test),index = unique_values.index,columns = unique_values.index)\n",
    "# con_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kappa, f1_score for non-stratified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #train f1_score\n",
    "# f1_Score = metrics.f1_score(predicted_train,y_train,average = 'weighted')\n",
    "# f1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #test f1_score\n",
    "# f1_Score = metrics.f1_score(predicted_test,y_test,average = 'weighted')\n",
    "# f1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # kappa train accuracy\n",
    "# metrics.cohen_kappa_score(predicted_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # kappa test accuracy\n",
    "# metrics.cohen_kappa_score(predicted_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## misclassified classes and their samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# misclassified_data = y_test[predicted_test != y_test].value_counts()\n",
    "# misclassify_data = pd.DataFrame(misclassified_data)\n",
    "# misclassify_data.columns = ['misclassified no of samples']\n",
    "# misclassify_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## accuracy per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perc_acc = pd.DataFrame(1-(misclassify_data['misclassified no of samples']/unique_values['test_unique_values']))\n",
    "# perc_acc.columns = ['percentage accuracy per class']\n",
    "# perc_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
